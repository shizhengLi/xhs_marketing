"""
OpenAI GPTåˆ†ææœåŠ¡ - åˆ†æå°çº¢ä¹¦çƒ­ç‚¹å†…å®¹
ç»“åˆGPT-4o-miniåˆ†ææ–‡å­—å†…å®¹ï¼Œè±†åŒ…Arkåˆ†æè§†é¢‘å†…å®¹
"""
import os
import json
import logging
from typing import List, Dict, Any
from datetime import datetime

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("OpenAIåº“æœªå®‰è£…ï¼ŒGPTåˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from volcenginesdkarkruntime import Ark
    ARK_AVAILABLE = True
except ImportError:
    ARK_AVAILABLE = False
    logging.warning("Arkåº“æœªå®‰è£…ï¼Œè±†åŒ…è§†é¢‘åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

logger = logging.getLogger(__name__)

class OpenAIAnalysisService:
    """OpenAI GPTåˆ†ææœåŠ¡"""

    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–é…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.api_base = os.getenv("OPENAI_API_BASE")
        self.ark_api_key = os.getenv("ARK_API_KEY")
        # æ”¯æŒå¤šç§æ¨¡å‹é…ç½®ç¯å¢ƒå˜é‡åç§°
        self.model = os.getenv("OPENAI_MODEL") or os.getenv("AI_MODEL", "gpt-4o-mini")
        self.client = None
        self.ark_client = None

        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if not self.api_key:
            logger.warning(
                "è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡\n"
                "å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¾ç½®:\n"
                "1. åœ¨ backend/.env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=your-key-here\n"
                "2. æˆ–è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡: export OPENAI_API_KEY=your-key-here"
            )
        if not self.api_base:
            logger.warning(
                "è¯·è®¾ç½® OPENAI_API_BASE ç¯å¢ƒå˜é‡\n"
                "å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¾ç½®:\n"
                "1. åœ¨ backend/.env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_BASE=https://api.openai.com/v1\n"
                "2. æˆ–è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡: export OPENAI_API_BASE=https://api.openai.com/v1"
            )
        if not self.ark_api_key:
            logger.warning(
                "è¯·è®¾ç½® ARK_API_KEY ç¯å¢ƒå˜é‡ç”¨äºè±†åŒ…è§†é¢‘åˆ†æ\n"
                "å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¾ç½®:\n"
                "1. åœ¨ backend/.env æ–‡ä»¶ä¸­æ·»åŠ : ARK_API_KEY=your-key-here\n"
                "2. æˆ–è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡: export ARK_API_KEY=your-key-here"
            )

        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆç”¨äºæ–‡å­—åˆ†æï¼‰
        if OPENAI_AVAILABLE and self.api_key and self.api_base:
            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url=self.api_base
                )
                logger.info(f"OpenAIæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model}")
            except Exception as e:
                logger.error(f"OpenAIåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        else:
            logger.warning("OpenAIæœåŠ¡ä¸å¯ç”¨ï¼šæœªé…ç½®API Keyã€API Baseæˆ–æœªå®‰è£…åº“")

        # åˆ›å»ºè±†åŒ…Arkå®¢æˆ·ç«¯ï¼ˆç”¨äºè§†é¢‘åˆ†æï¼‰
        if ARK_AVAILABLE and self.ark_api_key:
            try:
                self.ark_client = Ark(
                    base_url='https://ark.cn-beijing.volces.com/api/v3',
                    api_key=self.ark_api_key
                )
                logger.info("è±†åŒ…ArkæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œç”¨äºè§†é¢‘å†…å®¹åˆ†æ")
            except Exception as e:
                logger.error(f"è±†åŒ…Arkåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        else:
            logger.warning("è±†åŒ…ArkæœåŠ¡ä¸å¯ç”¨ï¼šæœªé…ç½®ARK_API_KEYæˆ–æœªå®‰è£…volcenginesdkarkruntimeåº“")

    def analyze_trending_content(self, posts: List[Dict[str, Any]], keyword: str) -> Dict[str, Any]:
        """
        æ·±åº¦åˆ†æçƒ­ç‚¹å†…å®¹ï¼ŒæŒ‰å…³é”®è¯é¢†åŸŸè¿›è¡Œä¸“ä¸šåˆ†æ

        Args:
            posts: çƒ­ç‚¹å¸–å­åˆ—è¡¨
            keyword: å…³é”®è¯

        Returns:
            åˆ†æç»“æœ
        """
        if not self.client:
            return {
                "success": False,
                "error": "OpenAIæœåŠ¡ä¸å¯ç”¨"
            }

        try:
            if len(posts) == 0:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯åˆ†æçš„å†…å®¹"
                }

            # å‡†å¤‡åˆ†ææ•°æ®
            content_data = self._prepare_enhanced_content_data(posts, keyword)

            # æ„å»ºæ·±åº¦åˆ†ææç¤º
            prompt = self._build_enhanced_analysis_prompt(content_data, keyword)

            # è°ƒç”¨é…ç½®çš„æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
            response = self.client.chat.completions.create(
                model=self.model,  # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„æ¨¡å‹
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æå¸ˆå’Œè¶‹åŠ¿ä¸“å®¶ï¼Œä¸“ç²¾äºï¼š
1. å°çº¢ä¹¦å¹³å°å†…å®¹ç”Ÿæ€æ·±åº¦åˆ†æ
2. ç”¨æˆ·è¡Œä¸ºå’Œäº’åŠ¨æ¨¡å¼æ´å¯Ÿ
3. è·¨é¢†åŸŸè¶‹åŠ¿å‘ç°å’Œæœºä¼šè¯†åˆ«
4. å•†ä¸šå˜ç°ç­–ç•¥å’Œå†…å®¹åˆ›ä½œæŒ‡å¯¼

ä½ çš„åˆ†æé£æ ¼ï¼šæ•°æ®é©±åŠ¨ã€æ´å¯Ÿæ·±åˆ»ã€å»ºè®®å®ç”¨ã€å‰ç»æ€§å¼ºã€‚"""
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2500
            )

            analysis_text = response.choices[0].message.content

            # è§£æåˆ†æç»“æœ
            analysis_result = self._parse_enhanced_analysis_result(analysis_text, keyword, posts)

            return {
                "success": True,
                "keyword": keyword,
                "analysis": analysis_result,
                "analyzed_count": len(posts),
                "analysis_date": datetime.now().isoformat(),
                "model_used": self.model
            }

        except Exception as e:
            logger.error(f"GPTåˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

    def _prepare_enhanced_content_data(self, posts: List[Dict[str, Any]], keyword: str) -> Dict[str, Any]:
        """å‡†å¤‡å¢å¼ºçš„ç”¨äºåˆ†æçš„å†…å®¹æ•°æ®"""
        # æŒ‰äº’åŠ¨æ•°æ’åºï¼Œè®¡ç®—ç»¼åˆçƒ­åº¦
        def calculate_engagement_score(post):
            return post.get('likes', 0) * 1.0 + post.get('collects', 0) * 1.5 + post.get('comments', 0) * 2.0

        sorted_posts = sorted(posts, key=calculate_engagement_score, reverse=True)[:15]

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_likes = sum(p.get('likes', 0) for p in posts)
        total_comments = sum(p.get('comments', 0) for p in posts)
        avg_likes = total_likes // len(posts) if posts else 0
        avg_comments = total_comments // len(posts) if posts else 0

        content_data = {
            "keyword": keyword,
            "total_posts": len(posts),
            "avg_likes": avg_likes,
            "avg_comments": avg_comments,
            "top_posts": []
        }

        for post in sorted_posts:
            content_data["top_posts"].append({
                "title": post.get('title', ''),
                "author": post.get('author', ''),
                "likes": post.get('likes', 0),
                "collects": post.get('collects', 0),
                "comments": post.get('comments', 0),
                "shares": post.get('shares', 0),
                "content": post.get('content', '')[:300],  # å¢åŠ åˆ°300å­—ç¬¦
                "engagement_score": calculate_engagement_score(post)
            })

        return content_data

    def _build_enhanced_analysis_prompt(self, content_data: Dict[str, Any], keyword: str) -> str:
        """æ„å»ºå¢å¼ºçš„GPTåˆ†ææç¤º"""
        prompt = f"""
# ğŸ¯ å°çº¢ä¹¦"{keyword}"é¢†åŸŸæ·±åº¦åˆ†æä»»åŠ¡

## ğŸ“Š æ•°æ®æ¦‚å†µ
- **åˆ†æé¢†åŸŸ**: {keyword}
- **æ•°æ®è§„æ¨¡**: {content_data['total_posts']}æ¡å†…å®¹
- **å¹³å‡äº’åŠ¨**: ç‚¹èµ{content_data['avg_likes']} | è¯„è®º{content_data['avg_comments']}
- **åˆ†ææ ·æœ¬**: TOP{len(content_data['top_posts'])}é«˜çƒ­åº¦å†…å®¹

## ğŸ”¥ çƒ­é—¨å†…å®¹æ•°æ®
```json
{json.dumps(content_data['top_posts'], ensure_ascii=False, indent=2)}
```

## ğŸª æ·±åº¦åˆ†æè¦æ±‚

è¯·ä»¥å°çº¢ä¹¦å†…å®¹ä¸“å®¶çš„èº«ä»½ï¼Œä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±åº¦åˆ†æï¼š

### 1. ğŸŒŸ å†…å®¹è¶‹åŠ¿è¯†åˆ«
- å½“å‰{keyword}é¢†åŸŸçš„3-5ä¸ªæ ¸å¿ƒçƒ­ç‚¹è¯é¢˜
- æ–°å…´è¶‹åŠ¿å’Œçˆ†å‘æ€§è¯é¢˜ï¼ˆå¦‚æœ‰ï¼‰
- å†…å®¹å½¢å¼åå¥½ï¼ˆå›¾æ–‡/è§†é¢‘/åˆé›†ç­‰ï¼‰

### 2. ğŸ‘¥ ç”¨æˆ·ç”»åƒä¸åå¥½
- ç›®æ ‡ç”¨æˆ·ç¾¤ä½“ç‰¹å¾ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€å…´è¶£ï¼‰
- ç”¨æˆ·ç—›ç‚¹å’Œéœ€æ±‚åˆ†æ
- é«˜äº’åŠ¨å†…å®¹çš„å…±åŒç‰¹å¾

### 3. ğŸ’¡ å†…å®¹åˆ›ä½œå¯†ç 
- æ ‡é¢˜å’Œæ–‡æ¡ˆçš„æˆåŠŸæ¨¡å¼
- è§†è§‰å‘ˆç°å’Œé£æ ¼åå¥½
- å‘å¸ƒæ—¶é—´å’Œæ—¶æœºç­–ç•¥

### 4. ğŸš€ å•†ä¸šä»·å€¼è¯„ä¼°
- æ½œåœ¨å•†ä¸šå˜ç°æœºä¼š
- å“ç‰Œåˆä½œå¯èƒ½æ€§
- äº§å“/æœåŠ¡æ¨å¹¿åˆ‡å…¥ç‚¹

### 5. ğŸ¯ ç«äº‰æ ¼å±€åˆ†æ
- å†…å®¹åˆ›ä½œè€…ç«äº‰å¼ºåº¦
- å·®å¼‚åŒ–æœºä¼šç‚¹
- é•¿æœŸå‘å±•æ½œåŠ›

## ğŸ“‹ è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONç»“æ„è¿”å›åˆ†æç»“æœï¼š
```json
{{
    "trend_highlights": [
        "æ ¸å¿ƒçƒ­ç‚¹1",
        "æ ¸å¿ƒçƒ­ç‚¹2",
        "æ ¸å¿ƒçƒ­ç‚¹3"
    ],
    "emerging_trends": [
        "æ–°å…´è¶‹åŠ¿1",
        "æ–°å…´è¶‹åŠ¿2"
    ],
    "user_persona": {{
        "target_audience": "ç›®æ ‡ç”¨æˆ·æè¿°",
        "pain_points": ["ç—›ç‚¹1", "ç—›ç‚¹2"],
        "engagement_drivers": ["é©±åŠ¨å› ç´ 1", "é©±åŠ¨å› ç´ 2"]
    }},
    "content_success_patterns": {{
        "title_patterns": ["æ ‡é¢˜æ¨¡å¼1", "æ ‡é¢˜æ¨¡å¼2"],
        "visual_preferences": ["è§†è§‰åå¥½1"],
        "posting_strategy": "å‘å¸ƒç­–ç•¥å»ºè®®"
    }},
    "commercial_opportunities": [
        {{
            "opportunity": "æœºä¼šæè¿°",
            "feasibility": "high/medium/low",
            "estimated_value": "é¢„ä¼°ä»·å€¼"
        }}
    ],
    "competitive_insights": {{
        "competition_level": "high/medium/low",
        "differentiation_opportunities": ["å·®å¼‚åŒ–æœºä¼š1"],
        "long_term_potential": "é•¿æœŸæ½œåŠ›è¯„ä¼°"
    }},
    "actionable_recommendations": [
        "å…·ä½“å»ºè®®1",
        "å…·ä½“å»ºè®®2",
        "å…·ä½“å»ºè®®3"
    ],
    "strategic_summary": "æ€»ç»“æ€§æˆ˜ç•¥æ´å¯Ÿ"
}}
```

âš ï¸ é‡è¦æé†’ï¼š
- åŸºäºå®é™…æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦å‡­ç©ºè‡†é€ 
- å‘ç°çœŸæ­£çš„æ´å¯Ÿï¼Œè€Œéæ³›æ³›è€Œè°ˆ
- å»ºè®®è¦å…·ä½“å¯æ‰§è¡Œ
- è€ƒè™‘å°çº¢ä¹¦å¹³å°ç‰¹æ€§å’Œç”¨æˆ·ä¹ æƒ¯
"""

        return prompt

    def _parse_enhanced_analysis_result(self, analysis_text: str, keyword: str, posts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è§£æå¢å¼ºçš„GPTåˆ†æç»“æœ"""
        try:
            # å°è¯•æå–JSON
            json_start = analysis_text.find('{')
            json_end = analysis_text.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                json_str = analysis_text[json_start:json_end]
                result = json.loads(json_str)

                # æ·»åŠ åˆ†æå…ƒæ•°æ®
                result['keyword'] = keyword
                result['analysis_date'] = datetime.now().isoformat()
                result['data_sample_size'] = len(posts)

                # æ·»åŠ æ•°æ®è´¨é‡è¯„åˆ†
                if len(posts) >= 10:
                    result['data_quality'] = 'high'
                elif len(posts) >= 5:
                    result['data_quality'] = 'medium'
                else:
                    result['data_quality'] = 'low'

                return result
            else:
                # å¦‚æœæ— æ³•æå–JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                return {
                    "raw_analysis": analysis_text,
                    "keyword": keyword,
                    "analysis_date": datetime.now().isoformat(),
                    "data_sample_size": len(posts),
                    "parsing_failed": True
                }
        except Exception as e:
            logger.error(f"è§£æåˆ†æç»“æœå¤±è´¥: {str(e)}")
            return {
                "raw_analysis": analysis_text,
                "keyword": keyword,
                "analysis_date": datetime.now().isoformat(),
                "data_sample_size": len(posts),
                "parse_error": str(e),
                "parsing_failed": True
            }

    def generate_comprehensive_daily_report(self, all_analyses: List[Dict[str, Any]], report_date: str = None) -> str:
        """
        ç”Ÿæˆç»¼åˆæ€§æ¯æ—¥çƒ­ç‚¹åˆ†ææŠ¥å‘Šï¼ŒæŒ‰å…³é”®è¯é¢†åŸŸåˆ†åˆ«åˆ†æ

        Args:
            all_analyses: æ‰€æœ‰å…³é”®è¯çš„åˆ†æç»“æœ
            report_date: æŠ¥å‘Šæ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©

        Returns:
            è¯¦ç»†æŠ¥å‘Šæ–‡æœ¬
        """
        if not self.client:
            return "âŒ OpenAIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š"

        try:
            if not all_analyses:
                return "âŒ æ²¡æœ‰åˆ†ææ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š"

            # è¿‡æ»¤æ‰åˆ†æå¤±è´¥çš„ç»“æœ
            valid_analyses = [a for a in all_analyses if a.get('success', False)]

            if not valid_analyses:
                return "âŒ æ‰€æœ‰åˆ†æéƒ½å¤±è´¥äº†ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š"

            # æ„å»ºæŠ¥å‘Šæç¤º
            prompt = self._build_comprehensive_report_prompt(valid_analyses, report_date)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç¤¾äº¤åª’ä½“å†…å®¹ç­–ç•¥å¸ˆå’Œè¶‹åŠ¿æŠ¥å‘Šä¸“å®¶ã€‚
ä½ æ“…é•¿ï¼š
1. æ•´åˆå¤šé¢†åŸŸæ•°æ®å½¢æˆå…¨å±€æ´å¯Ÿ
2. å‘ç°è·¨é¢†åŸŸçš„å…±åŒè¶‹åŠ¿å’Œå·®å¼‚
3. æä¾›æˆ˜ç•¥æ€§çš„å•†ä¸šå»ºè®®
4. æ’°å†™ç»“æ„æ¸…æ™°ã€æ´å¯Ÿæ·±åˆ»çš„æŠ¥å‘Š

æŠ¥å‘Šé£æ ¼ï¼šä¸“ä¸šã€æ•°æ®é©±åŠ¨ã€æ´å¯Ÿæ·±åˆ»ã€å»ºè®®å…·ä½“ã€‚
æ ¼å¼è¦æ±‚ï¼šä½¿ç”¨Markdownæ ¼å¼ï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºã€‚"""
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=4000
            )

            report = response.choices[0].message.content

            return report

        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

    def _build_comprehensive_report_prompt(self, valid_analyses: List[Dict[str, Any]], report_date: str = None) -> str:
        """æ„å»ºç»¼åˆæ€§æŠ¥å‘Šç”Ÿæˆæç¤º"""
        if not report_date:
            report_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

        # æŒ‰å…³é”®è¯ç»„ç»‡åˆ†ææ•°æ®
        keywords_data = {}
        for analysis in valid_analyses:
            keyword = analysis.get('keyword', 'æœªçŸ¥é¢†åŸŸ')
            keywords_data[keyword] = analysis.get('analysis', {})

        prompt = f"""
# ğŸ“Š å°çº¢ä¹¦çƒ­ç‚¹è¶‹åŠ¿ç»¼åˆåˆ†ææŠ¥å‘Š

## ğŸ“… æŠ¥å‘Šä¿¡æ¯
- **æŠ¥å‘Šæ—¥æœŸ**: {report_date}
- **åˆ†æé¢†åŸŸæ•°**: {len(valid_analyses)}ä¸ª
- **åˆ†æé¢†åŸŸ**: {', '.join(keywords_data.keys())}

## ğŸ¯ åˆ†ææ•°æ®
```json
{json.dumps(keywords_data, ensure_ascii=False, indent=2)}
```

## ğŸ“‹ æŠ¥å‘Šæ’°å†™è¦æ±‚

è¯·æ’°å†™ä¸€ä»½ä¸“ä¸šçš„å°çº¢ä¹¦çƒ­ç‚¹è¶‹åŠ¿åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

### 1. ğŸª æ‰§è¡Œæ‘˜è¦
- æ•´ä½“è¶‹åŠ¿æ¦‚è¿°ï¼ˆ1-2æ®µï¼‰
- æ ¸å¿ƒå‘ç°ï¼ˆ3-5ä¸ªå…³é”®æ´å¯Ÿï¼‰
- æˆ˜ç•¥å»ºè®®æ‘˜è¦

### 2. ğŸŒ åˆ†é¢†åŸŸæ·±åº¦åˆ†æ
å¯¹æ¯ä¸ªå…³é”®è¯é¢†åŸŸè¿›è¡Œè¯¦ç»†åˆ†æï¼š
- **{', '.join(keywords_data.keys())}**

æ¯ä¸ªé¢†åŸŸåŒ…å«ï¼š
- é¢†åŸŸçƒ­ç‚¹å’Œè¶‹åŠ¿
- ç”¨æˆ·ç”»åƒå’Œåå¥½
- å•†ä¸šæœºä¼šè¯„ä¼°
- åˆ›ä½œå»ºè®®

### 3. ğŸ” è·¨é¢†åŸŸå¯¹æ¯”åˆ†æ
- ä¸åŒé¢†åŸŸçš„å…±åŒè¶‹åŠ¿
- å„é¢†åŸŸçš„ç‹¬ç‰¹ç‰¹å¾
- å·®å¼‚åŒ–ç­–ç•¥æœºä¼š

### 4. ğŸ’¡ ç»¼åˆæˆ˜ç•¥å»ºè®®
- å†…å®¹åˆ›ä½œç­–ç•¥
- å•†ä¸šå˜ç°è·¯å¾„
- é•¿æœŸå‘å±•è§„åˆ’

### 5. ğŸ“ˆ æ•°æ®æ´å¯Ÿæ€»ç»“
- æ•°æ®è´¨é‡è¯„ä¼°
- å…³é”®æ•°æ®æŒ‡æ ‡
- åç»­è·Ÿè¸ªå»ºè®®

## ğŸ¨ æŠ¥å‘Šæ ¼å¼è¦æ±‚ï¼š
```markdown
# ğŸ¯ å°çº¢ä¹¦çƒ­ç‚¹è¶‹åŠ¿ç»¼åˆåˆ†ææŠ¥å‘Š
**ğŸ“… {report_date}**

## 1. ğŸª æ‰§è¡Œæ‘˜è¦
[æ•´ä½“è¶‹åŠ¿æ¦‚è¿°]

### ğŸ”‘ æ ¸å¿ƒå‘ç°
1. [å…³é”®æ´å¯Ÿ1]
2. [å…³é”®æ´å¯Ÿ2]
...

## 2. ğŸŒ åˆ†é¢†åŸŸæ·±åº¦åˆ†æ

### ğŸ“Š é¢†åŸŸä¸€ï¼š[å…³é”®è¯1]
#### ğŸŒŸ è¶‹åŠ¿æ´å¯Ÿ
[å…·ä½“åˆ†æ]

#### ğŸ‘¥ ç”¨æˆ·ç‰¹å¾
[ç”¨æˆ·ç”»åƒ]

#### ğŸš€ å•†ä¸šæœºä¼š
[æœºä¼šåˆ†æ]

#### ğŸ’¡ åˆ›ä½œå»ºè®®
[å…·ä½“å»ºè®®]

### ğŸ“Š é¢†åŸŸäºŒï¼š[å…³é”®è¯2]
[åŒæ ·ç»“æ„...]

## 3. ğŸ” è·¨é¢†åŸŸå¯¹æ¯”åˆ†æ
[è·¨é¢†åŸŸåˆ†æ]

## 4. ğŸ’¡ ç»¼åˆæˆ˜ç•¥å»ºè®®
[æˆ˜ç•¥å»ºè®®]

## 5. ğŸ“ˆ æ•°æ®æ´å¯Ÿæ€»ç»“
[æ•°æ®æ€»ç»“]

---
*æœ¬æŠ¥å‘ŠåŸºäºå°çº¢ä¹¦å¹³å°çœŸå®æ•°æ®ï¼Œç”±GPT-4o-miniæ™ºèƒ½åˆ†æç”Ÿæˆ*
```

âš ï¸ é‡è¦æé†’ï¼š
- ä¿æŒä¸“ä¸šæ€§å’Œæ´å¯Ÿæ·±åº¦
- ç¡®ä¿æ¯ä¸ªé¢†åŸŸéƒ½æœ‰ç‹¬ç«‹å®Œæ•´çš„åˆ†æ
- çªå‡ºè·¨é¢†åŸŸçš„æ¯”è¾ƒå’Œç»¼åˆæ´å¯Ÿ
- å»ºè®®è¦å…·ä½“å¯æ“ä½œ
- ç”¨æ•°æ®å’Œå‘ç°æ”¯æ’‘è§‚ç‚¹
- ä½¿ç”¨é€‚å½“çš„emojiå¢å¼ºå¯è¯»æ€§
"""

        return prompt

    def analyze_trending_content_with_video(self, posts: List[Dict[str, Any]], keyword: str) -> Dict[str, Any]:
        """
        æ·±åº¦åˆ†æçƒ­ç‚¹å†…å®¹ï¼Œç»“åˆæ–‡å­—å†…å®¹å’Œè§†é¢‘å†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æ
        ä½¿ç”¨æ··åˆAIæ¨¡å¼ï¼šGPT-4o-miniåˆ†ææ–‡å­—ï¼Œè±†åŒ…Arkåˆ†æè§†é¢‘

        Args:
            posts: çƒ­ç‚¹å¸–å­åˆ—è¡¨ï¼ŒåŒ…å«æ–‡å­—å’Œè§†é¢‘å†…å®¹
            keyword: å…³é”®è¯

        Returns:
            åˆ†æç»“æœ
        """
        if not self.client:
            return {
                "success": False,
                "error": "OpenAIæœåŠ¡ä¸å¯ç”¨"
            }

        try:
            if len(posts) == 0:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯åˆ†æçš„å†…å®¹"
                }

            # 1. é¦–å…ˆä½¿ç”¨è±†åŒ…Arkåˆ†æè§†é¢‘å†…å®¹
            video_insights = self._analyze_videos_with_ark(posts, keyword)

            # 2. å‡†å¤‡æ··åˆå†…å®¹åˆ†ææ•°æ®ï¼ˆåŒ…å«è§†é¢‘åˆ†æç»“æœï¼‰
            content_data = self._prepare_mixed_content_data(posts, keyword, video_insights)

            # 3. ä½¿ç”¨GPT-4o-miniè¿›è¡Œç»¼åˆåˆ†æ
            prompt = self._build_mixed_content_prompt(content_data, keyword)

            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # GPT-4o-miniç”¨äºç»¼åˆåˆ†æ
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå¸ˆå’Œè¥é”€ç­–ç•¥ä¸“å®¶ï¼Œæ“…é•¿ä»ç”¨æˆ·ç”Ÿæˆå†…å®¹ä¸­æå–æ·±åº¦æ´å¯Ÿå’Œå•†ä¸šä»·å€¼ã€‚
ä½ ç‰¹åˆ«æ“…é•¿ï¼š
1. åˆ†ææ–‡å­—å†…å®¹å’Œè§†é¢‘å†…å®¹çš„å·®å¼‚ä¸ä¼˜åŠ¿
2. ä»è§†é¢‘AIåˆ†æç»“æœä¸­æç‚¼ç”¨æˆ·çœŸå®éœ€æ±‚
3. åŸºäºæ•°æ®é©±åŠ¨çš„å†…å®¹ç­–ç•¥å»ºè®®
4. è·¨å½¢å¼å†…å®¹çš„è¡¨ç°åŠ›åˆ†æ"""
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=3000
            )

            analysis_text = response.choices[0].message.content

            # è§£æåˆ†æç»“æœ
            analysis_result = self._parse_mixed_content_analysis(analysis_text, keyword, posts)

            return {
                "success": True,
                "analysis": analysis_result,
                "raw_analysis": analysis_text,
                "analysis_date": datetime.now().isoformat(),
                "model_used": "GPT-4o-mini + è±†åŒ…Ark",
                "posts_analyzed": len(posts),
                "video_content_included": content_data['has_video_content']
            }

        except Exception as e:
            logger.error(f"æ··åˆAIåˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def _analyze_videos_with_ark(self, posts: List[Dict[str, Any]], keyword: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è±†åŒ…Arkåˆ†æè§†é¢‘å†…å®¹

        Args:
            posts: å¸–å­åˆ—è¡¨
            keyword: å…³é”®è¯

        Returns:
            è§†é¢‘åˆ†æç»“æœåˆ—è¡¨
        """
        video_insights = []

        if not self.ark_client:
            logger.warning("è±†åŒ…Arkä¸å¯ç”¨ï¼Œè·³è¿‡è§†é¢‘æ·±åº¦åˆ†æ")
            return video_insights

        for post in posts:
            if post.get('has_video') and post.get('video_url'):
                try:
                    # ä½¿ç”¨è±†åŒ…Arkåˆ†æè§†é¢‘ï¼ˆå‚è€ƒtest_video.pyçš„æ–¹æ³•ï¼‰
                    response = self.ark_client.responses.create(
                        model="doubao-seed-1-8-251228",
                        input=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "input_video",
                                        "video_url": post['video_url'],
                                        "fps": 1
                                    },
                                    {
                                        "type": "input_text",
                                        "text": f"è¯·åˆ†æè¿™ä¸ªå…³äº'{keyword}'çš„è§†é¢‘å†…å®¹ã€‚æè¿°è§†é¢‘ä¸­çš„{keyword}ç›¸å…³äº§å“å¤–è§‚ã€æè´¨ã€ä½¿ç”¨åœºæ™¯ã€è®¾è®¡ç‰¹ç‚¹ï¼Œä»¥åŠç”¨æˆ·å¯èƒ½çš„éœ€æ±‚å’Œåå¥½ã€‚"
                                    }
                                ],
                            }
                        ]
                    )

                    # æå–è±†åŒ…çš„åˆ†æç»“æœ
                    if response and hasattr(response, 'output'):
                        insight_text = ""
                        for output_item in response.output:
                            if hasattr(output_item, 'type') and output_item.type == 'message':
                                if hasattr(output_item, 'content') and output_item.content:
                                    for content_item in output_item.content:
                                        if hasattr(content_item, 'text') and content_item.text:
                                            insight_text = content_item.text
                                            break

                        video_insights.append({
                            'post_id': post.get('url', ''),
                            'video_url': post['video_url'],
                            'ark_analysis': insight_text,
                            'title': post.get('title', '')
                        })
                        logger.info(f"æˆåŠŸä½¿ç”¨è±†åŒ…Arkåˆ†æè§†é¢‘: {post.get('title', '')}")

                except Exception as e:
                    logger.error(f"è±†åŒ…Arkåˆ†æè§†é¢‘å¤±è´¥: {str(e)}")
                    # å¦‚æœè±†åŒ…åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å·²æœ‰çš„video_content
                    if post.get('video_content'):
                        video_insights.append({
                            'post_id': post.get('url', ''),
                            'video_url': post['video_url'],
                            'ark_analysis': post['video_content'],  # ä½¿ç”¨å·²æœ‰çš„åˆ†æç»“æœ
                            'title': post.get('title', ''),
                            'fallback': True
                        })

        return video_insights

    def _prepare_mixed_content_data(self, posts: List[Dict[str, Any]], keyword: str, video_insights: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å‡†å¤‡æ··åˆå†…å®¹åˆ†ææ•°æ®ï¼ˆæ–‡å­—+è§†é¢‘ï¼‰

        Args:
            posts: å¸–å­åˆ—è¡¨
            keyword: å…³é”®è¯
            video_insights: è±†åŒ…Arkè§†é¢‘åˆ†æç»“æœ
        """
        text_posts = []
        video_posts = []
        total_engagement = 0

        # åˆ›å»ºvideo_insightsçš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸
        insight_dict = {}
        if video_insights:
            for insight in video_insights:
                insight_dict[insight['post_id']] = insight

        for post in posts:
            engagement_score = post.get('likes', 0) + post.get('comments', 0) + post.get('shares', 0)
            total_engagement += engagement_score

            post_info = {
                'title': post.get('title', ''),
                'author': post.get('author', ''),
                'likes': post.get('likes', 0),
                'comments': post.get('comments', 0),
                'shares': post.get('shares', 0),
                'engagement_score': engagement_score,
                'url': post.get('url', '')
            }

            # åŒºåˆ†çº¯æ–‡å­—å†…å®¹å’Œè§†é¢‘å†…å®¹
            if post.get('has_video'):
                # è§†é¢‘å†…å®¹ï¼šä¼˜å…ˆä½¿ç”¨è±†åŒ…Arkåˆ†æï¼Œå¦åˆ™ä½¿ç”¨å·²æœ‰çš„video_content
                ark_analysis = insight_dict.get(post.get('url', ''), {}).get('ark_analysis', '')
                if ark_analysis:
                    post_info['video_analysis'] = ark_analysis
                else:
                    post_info['video_analysis'] = post.get('video_content', '')

                post_info['original_text'] = post.get('content', '')
                video_posts.append(post_info)
            else:
                # çº¯æ–‡å­—å†…å®¹
                post_info['content'] = post.get('content', '')
                text_posts.append(post_info)

        return {
            'keyword': keyword,
            'total_posts': len(posts),
            'text_posts_count': len(text_posts),
            'video_posts_count': len(video_posts),
            'has_video_content': len(video_posts) > 0,
            'total_engagement': total_engagement,
            'avg_engagement': total_engagement / len(posts) if posts else 0,
            'text_posts': text_posts[:8],  # é™åˆ¶æ•°é‡é¿å…tokenè¶…é™
            'video_posts': video_posts[:8],  # é™åˆ¶æ•°é‡é¿å…tokenè¶…é™
            'ark_analysis_count': len([insight for insight in video_insights if not insight.get('fallback')])
        }

    def _build_mixed_content_prompt(self, content_data: Dict[str, Any], keyword: str) -> str:
        """
        æ„å»ºæ··åˆå†…å®¹åˆ†ææç¤ºè¯
        """
        prompt = f"""è¯·å¯¹"{keyword}"å…³é”®è¯çš„å°çº¢ä¹¦çƒ­ç‚¹å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æã€‚

## æ•°æ®æ¦‚å†µ
- æ€»å†…å®¹æ•°: {content_data['total_posts']}æ¡
- çº¯æ–‡å­—å†…å®¹: {content_data['text_posts_count']}æ¡
- è§†é¢‘å†…å®¹: {content_data['video_posts_count']}æ¡
- æ€»äº’åŠ¨é‡: {content_data['total_engagement']}
- å¹³å‡äº’åŠ¨: {content_data['avg_engagement']:.0f}

## çº¯æ–‡å­—çƒ­ç‚¹å†…å®¹
"""

        # æ·»åŠ æ–‡å­—å†…å®¹
        for i, post in enumerate(content_data['text_posts'][:6], 1):
            prompt += f"""
### æ–‡å­—å†…å®¹ {i}
- æ ‡é¢˜: {post['title']}
- ä½œè€…: {post['author']}
- äº’åŠ¨é‡: {post['likes']}èµ {post['comments']}è¯„ {post['shares']}è½¬
- å†…å®¹: {post['content'][:300]}...
"""

        # æ·»åŠ è§†é¢‘å†…å®¹
        if content_data['video_posts']:
            prompt += f"""
## è§†é¢‘çƒ­ç‚¹å†…å®¹ï¼ˆå«AIè§†è§‰åˆ†æï¼‰
"""
            for i, post in enumerate(content_data['video_posts'][:6], 1):
                prompt += f"""
### è§†é¢‘å†…å®¹ {i}
- æ ‡é¢˜: {post['title']}
- ä½œè€…: {post['author']}
- äº’åŠ¨é‡: {post['likes']}èµ {post['comments']}è¯„ {post['shares']}è½¬
- åŸå§‹æè¿°: {post.get('original_text', '')[:200]}...
- AIè§†é¢‘åˆ†æ: {post['video_analysis'][:400]}...
"""

        prompt += f"""

## åˆ†æè¦æ±‚
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æï¼š

1. **å†…å®¹è¶‹åŠ¿åˆ†æ**: ç»“åˆæ–‡å­—å’Œè§†é¢‘å†…å®¹ï¼Œåˆ†æå½“å‰"{keyword}"é¢†åŸŸçš„ä¸»è¦çƒ­ç‚¹è¶‹åŠ¿
2. **è§†é¢‘vsæ–‡å­—å¯¹æ¯”**: åˆ†æè§†é¢‘å†…å®¹å’Œçº¯æ–‡å­—å†…å®¹åœ¨è¡¨ç°åŠ›ã€ç”¨æˆ·åé¦ˆæ–¹é¢çš„å·®å¼‚
3. **ç”¨æˆ·åå¥½æ´å¯Ÿ**: åŸºäºè§†é¢‘AIåˆ†æç»“æœï¼Œæç‚¼ç”¨æˆ·å¯¹"{keyword}"ç›¸å…³å†…å®¹çš„çœŸå®éœ€æ±‚å’Œåå¥½
4. **çˆ†æ¬¾è¦ç´ æç‚¼**: ä»é«˜äº’åŠ¨å†…å®¹ä¸­æå–æˆåŠŸè¦ç´ ï¼Œç‰¹åˆ«å…³æ³¨è§†é¢‘å†…å®¹çš„è§†è§‰å†²å‡»ç‚¹
5. **å†…å®¹æœºä¼šè¯†åˆ«**: æŒ‡å‡ºå½“å‰å†…å®¹ç©ºç™½ç‚¹å’Œæœºä¼šç‚¹
6. **åˆ›ä½œç­–ç•¥å»ºè®®**: æä¾›å…·ä½“çš„å†…å®¹åˆ›ä½œå»ºè®®ï¼ŒåŒ…æ‹¬æ˜¯å¦ä¼˜å…ˆè€ƒè™‘è§†é¢‘å½¢å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "trend_overview": "è¶‹åŠ¿æ¦‚è¿°",
    "content_preferences": "ç”¨æˆ·åå¥½åˆ†æ",
    "video_vs_text": "è§†é¢‘vsæ–‡å­—å¯¹æ¯”åˆ†æ",
    "success_factors": ["æˆåŠŸè¦ç´ 1", "æˆåŠŸè¦ç´ 2"],
    "content_gaps": ["å†…å®¹ç©ºç™½ç‚¹1", "å†…å®¹ç©ºç™½ç‚¹2"],
    "creation_strategy": "åˆ›ä½œç­–ç•¥å»ºè®®",
    "key_insights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2"],
    "recommended_topics": ["æ¨èä¸»é¢˜1", "æ¨èä¸»é¢˜2"]
}}
"""

        return prompt

    def _parse_mixed_content_analysis(self, analysis_text: str, keyword: str, posts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è§£ææ··åˆå†…å®¹åˆ†æç»“æœ
        """
        try:
            # å°è¯•è§£æJSONç»“æœ
            json_start = analysis_text.find('{')
            json_end = analysis_text.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                json_str = analysis_text[json_start:json_end]
                parsed_result = json.loads(json_str)
                return parsed_result
            else:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                return {
                    "trend_overview": analysis_text[:500],
                    "raw_analysis": analysis_text
                }
        except Exception as e:
            logger.warning(f"è§£æåˆ†æç»“æœå¤±è´¥: {str(e)}")
            return {
                "trend_overview": analysis_text[:500],
                "raw_analysis": analysis_text
            }


# å…¨å±€å®ä¾‹
openai_service = OpenAIAnalysisService()


def test_openai_service():
    """æµ‹è¯•OpenAIæœåŠ¡é…ç½®"""
    print("=== æµ‹è¯•OpenAIåˆ†ææœåŠ¡é…ç½® ===\n")

    print(f"API Key: {'å·²é…ç½®' if openai_service.api_key else 'æœªé…ç½®'}")
    print(f"API Base: {openai_service.api_base or 'æœªé…ç½®'}")
    print(f"Model: {openai_service.model}")
    print(f"Client Status: {'å·²åˆå§‹åŒ–' if openai_service.client else 'æœªåˆå§‹åŒ–'}")

    if openai_service.client:
        print("\nâœ… OpenAIæœåŠ¡é…ç½®æ­£ç¡®ï¼")
        print(f"å°†ä½¿ç”¨æ¨¡å‹: {openai_service.model}")
        print(f"APIåœ°å€: {openai_service.api_base}")
    else:
        print("\nâŒ OpenAIæœåŠ¡é…ç½®æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")


if __name__ == "__main__":
    test_openai_service()